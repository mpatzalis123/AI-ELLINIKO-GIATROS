import yaml
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict
import os
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
import logging
from openai import OpenAI
import glob
from fastapi.responses import JSONResponse

logging.basicConfig(level=logging.INFO)

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise Exception("Set your OPENAI_API_KEY in the .env file!")

client = OpenAI(api_key=api_key)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "AI Patient API running."}

# Request models Î¼Îµ session_id
class ChatRequest(BaseModel):
    session_id: str = Field(..., description="Unique session identifier")
    message: str
    scenario: str

class FeedbackRequest(BaseModel):
    session_id: str = Field(..., description="Unique session identifier")
    scenario: str
    history: List[Dict[str, str]]  # [{role: "user"/"assistant", content: "..."}]

def load_scenarios():
    scenarios = {}
    yaml_files = glob.glob("scenarios/*.yaml")
    for filepath in yaml_files:
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
                case_id = data.get("case_id") or os.path.splitext(os.path.basename(filepath))[0]
                scenarios[case_id] = data
                logging.info(f"âœ… Loaded scenario: {case_id}")
        except Exception as e:
            logging.error(f"âŒ Failed to load {filepath}: {e}")
    return scenarios

scenarios = load_scenarios()

# conversation_histories[session_id][scenario_id] = [messages]
conversation_histories: Dict[str, Dict[str, List[Dict[str, str]]]] = {}

@app.get("/scenarios")
async def get_scenarios():
    result = []
    for scenario_id, data in scenarios.items():
        title = data.get("title") or scenario_id.replace("_", " ").title()
        result.append({"id": scenario_id, "title": title})
    return JSONResponse(content=result)

@app.post("/chat")
async def chat(request: ChatRequest):
    scenario_id = request.scenario
    session_id = request.session_id
    scenario = scenarios.get(scenario_id)

    if not scenario:
        raise HTTPException(status_code=400, detail=f"Scenario '{scenario_id}' not found.")

    if session_id not in conversation_histories:
        conversation_histories[session_id] = {}

    if scenario_id not in conversation_histories[session_id]:
        conversation_histories[session_id][scenario_id] = []

    conversation_history = conversation_histories[session_id][scenario_id]

    try:
        student_message = request.message
        logging.info(f"[{scenario_id}][{session_id}] Student: {student_message}")

        conversation_history.append({"role": "user", "content": student_message})

        ai_prompt = scenario.get("ai_prompt")
        if not ai_prompt:
            raise HTTPException(status_code=500, detail=f"Scenario '{scenario_id}' missing 'ai_prompt'.")

        messages = [{"role": "system", "content": ai_prompt}] + conversation_history

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=500,
            temperature=0.8
        )
       
        ai_reply = response.choices[0].message.content.strip()
        conversation_history.append({"role": "assistant", "content": ai_reply})

        logging.info(f"[{scenario_id}][{session_id}] AI Reply: {ai_reply[:60]}...")

        return {"reply": ai_reply}

    except Exception as e:
        logging.error(f"ğŸ”¥ Error in /chat [{scenario_id}][{session_id}]: {e}")
        raise HTTPException(status_code=500, detail="Internal server error.")

@app.post("/feedback")
async def get_feedback(request: FeedbackRequest):
    try:
        history_formatted = "\n".join(
            f"{'ÎÎ¿ÏƒÎ·Î»ÎµÏ…Ï„Î®Ï‚' if m['role'] == 'user' else 'Î‘ÏƒÎ¸ÎµÎ½Î®Ï‚'}: {m['content']}" for m in request.history
        )

        messages = [
            {
                "role": "system",
                "content": (
                    "Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ Î­Î¼Ï€ÎµÎ¹ÏÎ¿Ï‚ ÎºÎ±Î¹ ÎµÏ…Î³ÎµÎ½Î¹ÎºÏŒÏ‚ ÎºÎ»Î¹Î½Î¹ÎºÏŒÏ‚ Î±Î¾Î¹Î¿Î»Î¿Î³Î·Ï„Î®Ï‚. "
                    "Î‘Î½Î±Î»ÏÎµÎ¹Ï‚ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯ÎµÏ‚ Î¼ÎµÏ„Î±Î¾Ï Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î® (user) ÎºÎ±Î¹ Î±ÏƒÎ¸ÎµÎ½Î¿ÏÏ‚ (assistant) "
                    "ÎºÎ±Î¹ Î´Î¯Î½ÎµÎ¹Ï‚ Î®Ï€Î¹Î±, Î¶ÎµÏƒÏ„Î®, Î±Î½Î¸ÏÏÏ€Î¹Î½Î· ÎºÎ±Î¹ ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ® Î±Î½Î±Ï„ÏÎ¿Ï†Î¿Î´ÏŒÏ„Î·ÏƒÎ· ÏƒÏ„Î¿Î½ Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î®. "
                    "Î•Ï€Î¹ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹Ï‚ Ï„Î¹ Ï€Î®Î³Îµ ÎºÎ±Î»Î¬, Ï„Î¹ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î²ÎµÎ»Ï„Î¯Ï‰ÏƒÎ· ÎºÎ±Î¹ Ï€ÏÏ‚ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î³Î¯Î½ÎµÎ¹ Ï€Î¹Î¿ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÏŒÏ‚ ÏƒÏ„Î¿ Î¼Î­Î»Î»Î¿Î½. "
                    "Î•Î¬Î½ Î¿ Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î®Ï‚ ÎµÎºÏ†ÏÎ¬Î¶ÎµÏ„Î±Î¹ Î±Ï€ÏŒÏ„Î¿Î¼Î±, ÎµÎ¹ÏÏ‰Î½Î¹ÎºÎ¬ Î® Î±ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î±, ÎµÎ¾Î·Î³ÎµÎ¯Ï‚ Î³Î¹Î±Ï„Î¯ Î±Ï…Ï„ÏŒ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÏƒÏ‰ÏƒÏ„ÏŒ. "
                    "Î‘Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î±, Î±Î½ Î¿ Î±ÏƒÎ¸ÎµÎ½Î®Ï‚ ÎµÎ¯Î½Î±Î¹ Î´ÏÏƒÎºÎ¿Î»Î¿Ï‚ Î® Î±Î³ÎµÎ½Î®Ï‚, Î´ÎµÎ¯Ï‡Î½ÎµÎ¹Ï‚ Ï„ÏÏŒÏ€Î¿Ï…Ï‚ Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·Ï‚ Î¼Îµ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ. "
                    "Î— Î±Î½Î±Ï„ÏÎ¿Ï†Î¿Î´ÏŒÏ„Î·ÏƒÎ· Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ ÏƒÎµ Ï†Ï…ÏƒÎ¹ÎºÎ¬ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, Ï‡Ï‰ÏÎ¯Ï‚ bullets Î® Ï„Î¯Ï„Î»Î¿Ï…Ï‚, ÏƒÎ±Î½ Î½Î± Î¼Î¹Î»Î¬Ï‚ Î±Ï€ÎµÏ…Î¸ÎµÎ¯Î±Ï‚ ÏƒÏ„Î¿Î½ Ï†Î¿Î¹Ï„Î·Ï„Î® Î¼Îµ ÏƒÎµÎ²Î±ÏƒÎ¼ÏŒ."
                )
            },
            {
                "role": "user",
                "content": f"Î‘Î½Î±Ï„ÏÎ¿Ï†Î¿Î´ÏŒÏ„Î·ÏƒÎµ Ï„Î¿Î½ Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î® Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±:\n\n{history_formatted}"
            }
        ]

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=800,
            temperature=0.5
        )

        feedback = response.choices[0].message.content.strip()
        return {"feedback": feedback}

    except Exception as e:
        logging.error(f"ğŸ”¥ Error in /feedback: {e}")
        raise HTTPException(status_code=500, detail="Error generating feedback.")
    
from random import uniform, choice, randint

class PhysicalExamRequest(BaseModel):
    session_id: str
    scenario: str

async def get_ai_generated_exam(prompt: str) -> str:
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ Î²Î¿Î·Î¸ÏŒÏ‚ Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î¹ÎºÎ®Ï‚ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logging.error(f"ğŸ”¥ AI exam generation error: {e}")
        return "Î£Ï†Î¬Î»Î¼Î± AI"

import json  # Î’ÎµÎ²Î±Î¹ÏÏƒÎ¿Ï… ÏŒÏ„Î¹ ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î·Î½ Î±ÏÏ‡Î® Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï…

@app.post("/physical_exam")
async def physical_exam(request: PhysicalExamRequest):
    scenario_id = request.scenario
    scenario = scenarios.get(scenario_id)

    if not scenario:
        raise HTTPException(status_code=400, detail="Scenario not found.")

    scenario_yaml = yaml.dump(scenario, allow_unicode=True)
    prompt = (
        "ÎœÎµ Î²Î¬ÏƒÎ· Ï„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÏƒÎµÎ½Î¬ÏÎ¹Î¿ Î±ÏƒÎ¸ÎµÎ½Î¿ÏÏ‚ ÏƒÎµ YAML, Î³ÏÎ¬ÏˆÎµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï„Î·Ï‚ Î¦Î¥Î£Î™ÎšÎ—Î£ Î•ÎÎ•Î¤Î‘Î£Î—Î£ ÏƒÎµ JSON Î¼Î¿ÏÏ†Î®, "
        "Î¼Îµ Ï„Î± ÎµÎ¾Î®Ï‚ Ï€ÎµÎ´Î¯Î±: temperature_celsius, blood_pressure_mmHg (Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ subfields systolic ÎºÎ±Î¹ diastolic), "
        "pulse_bpm, oxygen_saturation_percent, neurological_status (consciousness_level, orientation, dizziness), "
        "musculoskeletal_system (pain_location, mobility, muscle_strength, swelling, bruising), "
        "skin_condition (color, temperature), psychological_state (emotional_state, cooperation). "
        "Î”ÏÏƒÎµ Î¼ÏŒÎ½Î¿ Ï„Î¿ JSON, Ï‡Ï‰ÏÎ¯Ï‚ ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î® ÎµÎ¾Î·Î³Î®ÏƒÎµÎ¹Ï‚.\n\n"
        f"{scenario_yaml}"
    )

    result_text = await get_ai_generated_exam(prompt)
    result_text = result_text.strip()

    # Î‘Î½ Î±ÏÏ‡Î¯Î¶ÎµÎ¹ ÎºÎ±Î¹ Ï„ÎµÎ»ÎµÎ¹ÏÎ½ÎµÎ¹ Î¼Îµ ```json ... ```
    if result_text.startswith("```"):
        result_text = result_text.strip("`").strip()
        if result_text.lower().startswith("json"):
            result_text = result_text[4:].strip()

    logging.info(f"Cleaned AI response:\n{result_text}")

    try:
        result_json = json.loads(result_text)
    except json.JSONDecodeError as e:
        logging.error(f"JSON decode error in physical_exam: {e}")
        raise HTTPException(status_code=500, detail="AI response JSON decode error.")

    return JSONResponse(content=result_json)

class DiagnosticTestsRequest(BaseModel):
    session_id: str
    scenario: str


from fastapi import HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import yaml

class DiagnosticTestsRequest(BaseModel):
    session_id: str
    scenario: str

@app.post("/diagnostic_tests")
async def diagnostic_tests(request: DiagnosticTestsRequest):
    scenario_id = request.scenario
    scenario = scenarios.get(scenario_id)

    if not scenario:
        raise HTTPException(status_code=400, detail="Scenario not found.")

    scenario_yaml = yaml.dump(scenario, allow_unicode=True)

    prompt = (
        "Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒÏ‚ Î¹Î±Ï„ÏÎ¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚. "
        "ÎœÎµ Î²Î¬ÏƒÎ· Ï„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÏƒÎµÎ½Î¬ÏÎ¹Î¿ Î±ÏƒÎ¸ÎµÎ½Î¿ÏÏ‚ (Î¼Î¿ÏÏ†Î® YAML), ÎµÏ€Î¯Î»ÎµÎ¾Îµ 4â€“6 ÎºÎ±Ï„Î¬Î»Î»Î·Î»ÎµÏ‚ Î´Î¹Î±Î³Î½Ï‰ÏƒÏ„Î¹ÎºÎ­Ï‚ ÎµÎ¾ÎµÏ„Î¬ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Î´ÏÏƒÎµ Î¼ÏŒÎ½Î¿ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¬ Ï„Î¿Ï…Ï‚.\n"
        "ÎœÎ·Î½ ÎµÎ¾Î·Î³ÎµÎ¯Ï‚ Ï„Î¯Ï€Î¿Ï„Î±. Î”ÏÏƒÎµ Î¼ÏŒÎ½Î¿ Ï„Î¹Î¼Î­Ï‚, ÏƒÎ±Î½ Î±Î½Î±Ï†Î¿ÏÎ¬ ÎµÏÎ³Î±ÏƒÏ„Î·ÏÎ¯Î¿Ï… Î® Î³Î½Ï‰Î¼Î¬Ï„ÎµÏ…ÏƒÎ· Î±Ï€ÎµÎ¹ÎºÎ¿Î½Î¹ÏƒÏ„Î¹ÎºÎ®Ï‚ ÎµÎ¾Î­Ï„Î±ÏƒÎ·Ï‚:\n\n"
        "**Î”ÎµÎ¯Î³Î¼Î± Î¼Î¿ÏÏ†Î®Ï‚:**\n"
        "Hb: 12.5 g/dL\nNa+: 138 mmol/L\nÎ‘ÎºÏ„Î¹Î½Î¿Î³ÏÎ±Ï†Î¯Î± Î¹ÏƒÏ‡Î¯Î¿Ï…: ÎšÎ¬Ï„Î±Î³Î¼Î± Ï…Ï€Î¿ÎºÎµÏ†Î±Î»Î¹ÎºÏŒ\nÎ—ÎšÎ“: Î¦Î»ÎµÎ²Î¿ÎºÎ¿Î¼Î²Î¹ÎºÏŒÏ‚ ÏÏ…Î¸Î¼ÏŒÏ‚\n\n"
        f"{scenario_yaml}"
    )

    result = await get_ai_generated_exam(prompt)
    print("ğŸ“¦ Diagnostic Test Result from AI:", result)

    return JSONResponse(content={"diagnostic_tests": result})

